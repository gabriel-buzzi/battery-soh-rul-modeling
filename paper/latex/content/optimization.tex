\section{Model Training and Optimization}
\label{sec:optimiation}
% Introducing the purpose and approach
This section describes the methodology for training and optimizing machine learning models to estimate the \acf{soh} and \acf{rul} of lithium-ion batteries, utilizing the features extracted from the dataset detailed in prior sections. Machine learning models are mathematical frameworks that identify patterns in data to predict outcomes, here mapping cycle features to \ac{soh} or \ac{rul}. Separate models were trained for \ac{soh} and \ac{rul} to avoid the complexity of joint estimation, focusing on generalization to cells not included in the training data, a critical requirement for applications such as battery management systems in electric vehicles or energy storage.

\subsection{Data Preparation}

% Describing train-test split
To ensure models generalize to unseen cells, the dataset of 124 cells was partitioned into training and test sets by randomly assigning entire cells, preventing any cell’s data from appearing in both sets. This approach mitigates data leakage, where models could exploit specific cell behaviors, leading to overly optimistic performance estimates. As presented in \cref{tab:splits_cells_summary}, the training set comprises 99 cells (80,618 cycles), and the test set includes 25 cells (18,480 cycles), adhering to an approximate 80:20 split, a standard practice to balance training data availability with robust evaluation.

\begin{table}[h]
    \centering
    \begin{tabular}{lrr}
        \toprule
        \textbf{Partition} & \textbf{No. of Cells} & \textbf{Total Samples (Cycles)} \\
        \midrule
        Training & 99 & 80618 \\
        Test     & 25 & 18480 \\
        \midrule
        \textbf{Total} & \textbf{124} & \textbf{99098} \\
        \bottomrule
    \end{tabular}
    \caption{Division of cells and total samples (cycles) into training and test sets.}
    \label{tab:splits_cells_summary}
\end{table}

% Describing feature scaling
Features were standardized using Standard Scaling, transforming each feature to have a mean of zero and a \acf{std} of one, based on statistics derived from the training set. Standardization ensures that features with different scales (e.g., voltage mean in volts vs. unitless kurtosis) contribute equally to model training, preventing bias toward larger-magnitude features. The scaling parameters from the training set were applied to the test set to maintain consistency, mirroring real-world scenarios where new data are processed using established parameters.

\subsection{Model Training and Evaluation}

% Describing model training
Model training entails optimizing internal parameters (weights) to minimize prediction errors on the training data. In this study, models learn to map cycle features (e.g., voltage mean, temperature \ac{std}) to \ac{soh} or \ac{rul}, minimizing the \acf{mse}, which quantifies the average squared difference between predicted and actual values, penalizing larger errors more heavily. Ground truth \ac{soh} and \ac{rul} values from all cycles of the 99 training cells were utilized, leveraging the dataset’s full-life data. In practical applications, full-life data may be unavailable, necessitating alternative approaches: (1) personalized models trained on early cycles of a specific cell for its future predictions, or (2) general models trained on early cycles from multiple cells, applicable to any cell at any stage. These methods are challenging, as early-cycle data may not capture aging patterns, a significant hurdle in battery research. Given the availability of full-life data, this study uses all cycles from training cells to assess feature and model performance for unseen cells, aligning with the objective of generalization.

% Describing model evaluation
Model performance was evaluated by generating predictions for the test set (25 unseen cells) and comparing them to ground truth \ac{soh} and \ac{rul} values using three metrics:

\begin{itemize}
    \item \textbf{\acf{mae}:} The average absolute difference between predicted and actual values, expressed in the target’s units (e.g., percentage for \ac{soh}, cycles for \ac{rul}).
    \begin{equation}
        \mathrm{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|
        \label{eq:mae}    
    \end{equation}
    
    \item \textbf{\acf{rmse}:} The square root of \ac{mse}, emphasizing larger errors while maintaining the target’s units.
    \begin{equation}
        \mathrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
        \label{eq:rmse}
    \end{equation}
    
    \item \textbf{\acf{r2}:} A metric from 0 to 1, indicating the proportion of variance in the target explained by the model, with 1 representing perfect predictions.
    \begin{equation}
        R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
        \label{eq:r2}
    \end{equation}
\end{itemize}

For inference, models require voltage, current, and temperature signals from a single full charge-discharge cycle, sampled at 1 Hz or higher, processed into the 15 features described previously.

\subsection{Hyperparameter Optimization}

% Defining hyperparameters and optimization approach
Machine learning models rely on hyperparameters—configurable settings that define their structure or learning process, such as the number of trees in a Random Forest or the learning rate in a neural network. Unlike model weights, hyperparameters are set prior to training and typically determined empirically. To automate this process, the Tree-structured Parzen Estimator (TPE) \cite{tpe_2011}, a Bayesian optimization algorithm, was employed. TPE iteratively samples from a predefined hyperparameter space, balancing exploration of new configurations with exploitation of previously successful ones.

To balance predictive accuracy and model generalization, a custom cost function was adopted, combining validation error and the generalization gap. The objective to minimize is defined as:

\begin{equation}
    \mathcal{L} = \mathrm{RMSE}_{\text{val}} + \frac{\left| \mathrm{RMSE}_{\text{train}} - \mathrm{RMSE}_{\text{val}} \right|}{\mathrm{RMSE}_{\text{val}}}
\end{equation}

where \(\text{RMSE}_{\text{val}}\) is the root mean square error on the validation folds, as defined by \cref{eq:rmse}, and the second term, \(\frac{|\text{RMSE}_{\text{train}} - \text{RMSE}_{\text{val}}|}{\text{RMSE}_{\text{val}}}\), represents the relative generalization gap between training and validation errors. This formulation penalizes both high validation errors and large discrepancies between training and validation performance, effectively discouraging overfitting while prioritizing predictive accuracy.

% Explaining cross-validation setup
Trials were evaluated using 5-fold Group K-Fold cross-validation. The 99 training cells were partitioned into 5 non-overlapping groups (based on cell ID), ensuring that all cycles from a given cell remained within the same fold to prevent data leakage. The final loss score is computed as the average of the defined loss across the 5 validation folds. \cref{fig:kfold} below illustrates the 5-fold cross-validation process adopted.

% Detailing early stopping strategy
To improve optimization efficiency and prevent unnecessary trials, an early stopping strategy was employed. If the cost does not improve for a fixed number of consecutive trials (patience = 10), the optimization process is halted. This ensures that exploration ceases when no meaningful improvement is observed.

\insertfigure{optimization/kfold}

% Data subset for optimization
\insertfigure{optimization/subset_distributions}

% Summarizing the methodology
This methodology leverages full-life data from training cells to develop models that generalize to unseen cells, addressing the challenges of \ac{soh} and \ac{rul} estimation. Through standardized features, multi-object optimization and hyperparameter optimization via TPE and cross-validation, the approach ensures robust predictions avoiding artificially inflated metrics due to overfitting.
