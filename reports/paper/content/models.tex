\section{Models}

Different types of algorithms have distinct properties and are capable of capturing statistical relationships between input (predictor) and output (target) variables in various ways. It is not possible to determine in advance which algorithm will be more or less suitable for a given problem, as this depends on the relationship between the data and the algorithm's capabilities. Generally, linear models can recognize simpler patterns (linear in nature), while nonlinear models can capture more complex relationships, albeit at the cost of requiring larger amounts of training data.
In addition to the type of model used, the ``settings'' of each model, known as hyperparameters, can also be varied. These are parameters not automatically learned during training but must be manually adjusted beforehand. Tuning these hyperparameters alters the model's properties, affecting the training process and the final inference performance.
In this context, the most common approach in the literature for selecting an algorithm and its hyperparameter settings for a specific problem is experimentation and comparison. Accordingly, this work evaluates a series of algorithms through a systematic hyperparameter tuning process to determine which algorithm best fits the problem's data, achieving the highest inference accuracy. The algorithms evaluated are listed below, followed by a brief explanation of their functionality, with the hyperparameter optimization process described in detail in subsequent sections.

\subsection{Elastic Net}

\subsection{KNN}

\subsection{Random Forest}

Random Forest is an ensemble method that combines multiple decision trees using bootstrap aggregating (bagging). Each tree is trained on a different bootstrap sample of the data, and at each split, only a random subset of variables is considered.
The final prediction is the average of all tree predictions: \begin{equation} \hat{y} = \frac{1}{B} \sum_{b=1}^{B} T_b(\mathbf{x}) \end{equation}
where $B$ is the number of trees, and $T_b$ is the $b$-th tree. This approach significantly reduces variance compared to individual trees while maintaining low bias. Random Forest offers strong out-of-the-box performance, is robust to outliers, and provides measures of variable importance.

\subsection{LightGBM}

LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework developed by Microsoft that uses tree-based algorithms. Unlike other boosting methods that grow trees level-wise, LightGBM grows trees leaf-wise, expanding the leaf that most reduces the loss.
The model combines multiple weak trees sequentially: 
\begin{equation} 
    F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \gamma_m h_m(\mathbf{x}) 
\end{equation}
where $h_m$ is the $m$-th tree, and $\gamma_m$ is its associated weight. LightGBM offers high computational efficiency, low memory usage, and good accuracy, making it particularly effective for large datasets. It includes optimizations such as continuous variable discretization and efficient parallelization.


% \insertfigure{fig_model}
% \inserttable{tbl_dataset}
