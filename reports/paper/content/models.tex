\subsection{Machine Learning Models}

In this study, we evaluated four machine learning regression models on our dataset to predict the target variable. The models include the Tweedie Regressor, K-Nearest Neighbors (KNN) Regressor, Extra Trees Regressor, and LightGBM Regressor. Each model was trained and tested using standard cross-validation techniques to ensure robustness. Below, we describe each model and its mathematical formulation.

\subsubsection{Tweedie Regressor}

The Tweedie Regressor is a generalized linear model tailored for response variables following a Tweedie distribution, suitable for datasets with zero-inflated or positively skewed data (e.g., insurance claims or rainfall). It models the expected value of the response variable \( y \) given features \( \mathbf{x} \) as:

\[
\mu(\mathbf{x}) = \exp(\mathbf{x}^T \boldsymbol{\beta}),
\]

where \( \boldsymbol{\beta} \) is the coefficient vector, and the model optimizes the Tweedie deviance loss function:

\[
\text{Deviance} = 2 \sum_{i=1}^n \left[ \frac{y_i^{2-p}}{(1-p)(2-p)} - \frac{y_i \mu_i^{1-p}}{1-p} + \frac{\mu_i^{2-p}}{2-p} \right],
\]

with \( p \) (where \( 1 < p < 2 \)) controlling the Tweedie distribution's variance-power relationship. This loss balances the fit for zero and non-zero values, making it effective for mixed distributions.

\subsubsection{K-Nearest Neighbors (KNN) Regressor}

The KNN Regressor predicts the target value for a test instance \( \mathbf{x} \) by averaging the target values of its \( k \) nearest neighbors in the feature space:

\[
\hat{y}(\mathbf{x}) = \frac{1}{k} \sum_{\mathbf{x}_i \in \mathcal{N}_k(\mathbf{x})} y_i,
\]

where \( \mathcal{N}_k(\mathbf{x}) \) is the set of \( k \) nearest neighbors based on a distance metric (e.g., Euclidean distance, \( d(\mathbf{x}, \mathbf{x}_i) = \sqrt{(\mathbf{x} - \mathbf{x}_i)^T (\mathbf{x} - \mathbf{x}_i)} \)). The prediction is a simple average, making KNN intuitive but sensitive to the choice of \( k \) and feature scaling.

\subsubsection{Extra Trees Regressor}

The Extra Trees Regressor is an ensemble method that constructs multiple decision trees with randomized splits and aggregates their predictions. For a test instance \( \mathbf{x} \), the prediction is:

\[
\hat{y}(\mathbf{x}) = \frac{1}{T} \sum_{t=1}^T h_t(\mathbf{x}),
\]

where \( T \) is the number of trees, and \( h_t(\mathbf{x}) \) is the prediction from the \( t \)-th tree. Each tree is built by selecting random features and split thresholds, reducing variance through averaging while maintaining computational efficiency compared to standard Random Forests.

\subsubsection{LightGBM Regressor}

The LightGBM Regressor is a gradient boosting framework that constructs an additive model of weak learners (decision trees):

\[
\hat{y}(\mathbf{x}) = \sum_{m=1}^M f_m(\mathbf{x}),
\]

where \( f_m(\mathbf{x}) \) is the \( m \)-th tree, and \( M \) is the number of boosting iterations. The model minimizes a loss function (e.g., mean squared error, \( L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 \)) by iteratively adding trees that reduce the gradient of the loss. LightGBM uses histogram-based splitting and leaf-wise growth for efficiency, making it scalable for large datasets.