\section{Introduction}

% Need to insert the citations to the papers on bibtex

Lithium-ion batteries have emerged as the dominant energy storage technology for applications ranging from electric vehicles to grid-scale storage systems, yet their performance degradation over time remains a critical challenge for operational reliability and economic viability. Accurate estimation of battery State of Health (SoH) and Remaining Useful Life (RUL) is essential for predictive maintenance, warranty management, and ensuring safe operation throughout the battery lifecycle. While numerous machine learning approaches have been proposed to address this challenge, significant gaps remain between laboratory demonstrations and practical deployment in real-world battery management systems.

The complexity of battery degradation mechanisms, which involve interconnected electrochemical, thermal, and mechanical processes, makes accurate prognosis particularly challenging. These degradation patterns manifest differently depending on operating conditions, usage profiles, and environmental factors, necessitating robust modeling approaches that can generalize across diverse scenarios. Recent advances in machine learning have shown promise in capturing these complex relationships; however, critical methodological limitations often compromise their real-world applicability.

A fundamental challenge in battery prognosis research is the reliance on controlled laboratory data that may not represent actual usage conditions. Many existing approaches depend on features extracted from complete charge-discharge cycles performed under constant current conditions—a scenario rarely encountered in practical applications where batteries experience partial, irregular charging patterns and dynamic load profiles. More critically, several methods rely on multiple subsequent constant-current–constant-voltage (CCCV) cycles to build predictive features or model trajectories. Such requirements impose strong assumptions about data availability that are unrealistic in practice, since real-world batteries are not cycled in long, uninterrupted CCCV sequences. In contrast, diagnostic opportunities in the field typically arise during maintenance or scheduled service, where at most a single complete CCCV cycle can be recorded.

Another pervasive issue in the literature is data leakage, where information from test datasets inadvertently influences model training through feature engineering or preprocessing steps. For instance, some methods apply signal decomposition techniques or normalization procedures to entire datasets before splitting them into training and test sets, allowing future information to contaminate the training process. Similarly, approaches that rely on absolute capacity measurements or State of Charge values as input features assume access to ground-truth information that is difficult or impossible to obtain reliably in operational settings, particularly for aged cells where capacity itself is the parameter being estimated.

The feature extraction strategies employed in many studies further limit practical deployment. Methods that require multi-cycle historical data or depend on specific portions of the charge-discharge curve selected based on capacity thresholds introduce circular dependencies and operational constraints. Additionally, the widespread use of constant current discharge data for model validation, while convenient for laboratory studies, does not reflect the variable and unpredictable discharge patterns encountered in real-world applications such as electric vehicles or consumer electronics.

This work addresses these limitations through a comprehensive framework for battery SoH and RUL estimation designed with practical deployment considerations at its core. Using the publicly available dataset from Severson et al. (2019), which contains cycling data from 124 commercial lithium iron phosphate batteries tested under various fast-charging conditions, we develop and validate machine learning models that generalize to unseen cells without requiring personalized training. Our approach employs statistical features extracted from a single complete CCCV cycle, eliminating dependencies on multi-cycle data or absolute capacity measurements. Crucially, this design mirrors how a diagnostic cycle could be performed in practice during regular maintenance, enabling inference without the unrealistic assumption of continuous CCCV cycling. We further implement rigorous data splitting strategies where entire cells are reserved for testing, ensuring no information leakage between training and evaluation sets.

The primary contributions of this work include: (1) a systematic analysis of feature extraction methods that rely only on directly measurable signals without assuming knowledge of absolute capacity or State of Charge, (2) implementation of multiple machine learning models (Tweedie Regressor, K-Nearest Neighbors, Extra Trees, and LightGBM) with careful hyperparameter optimization using cross-validation techniques that prevent data leakage, (3) comprehensive evaluation demonstrating model generalization to completely unseen batteries, and (4) critical assessment of the dataset's limitations and their implications for real-world deployment. By addressing the methodological weaknesses prevalent in existing approaches, this work provides a more realistic assessment of achievable performance in battery prognosis while establishing best practices for future research in this domain.
